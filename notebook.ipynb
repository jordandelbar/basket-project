{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "from itertools import chain\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score, \n",
    "                             roc_auc_score, \n",
    "                             roc_curve, \n",
    "                             auc, \n",
    "                             confusion_matrix)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"number_of_items\", \"price_of_basket\", \"items_ix\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can split our dataset into a train, validation and test set and transform it to numpy arrays\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(\"label\", axis=1).values, \n",
    "                                                    data[\"label\"].values,\n",
    "                                                    stratify=data[\"label\"].values,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=random_state)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, \n",
    "                                                y_test,\n",
    "                                                stratify=y_test,\n",
    "                                                test_size=0.5, \n",
    "                                                random_state=random_state)\n",
    "\n",
    "print(\"train size:\", len(x_train))\n",
    "print(\"validation size:\", len(x_val))\n",
    "print(\"test size:\", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x_train[:, subset_indices] = scaler.fit_transform(x_train[:, subset_indices])\n",
    "x_val[:, subset_indices] = scaler.transform(x_val[:, subset_indices])\n",
    "x_test[:, subset_indices] = scaler.transform(x_test[:, subset_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_set = set(chain.from_iterable(data[\"items_ix\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(list(items_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "items_train = pd.DataFrame(x_train[:, 2], columns=[\"basket_items\"])\n",
    "items_train[\"label\"] = y_train\n",
    "items_train = items_train.explode(\"basket_items\")\n",
    "\n",
    "# Validation\n",
    "items_val = pd.DataFrame(x_val[:, 2], columns=[\"basket_items\"])\n",
    "items_val[\"label\"] = y_val\n",
    "items_val = items_val.explode(\"basket_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super(EmbeddingsClassifier, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCELoss_class_weighted(weights):\n",
    "\n",
    "    def loss(input, target):\n",
    "        input = torch.clamp(input,min=1e-7,max=1-1e-7)\n",
    "        bce = - weights[1] * target * torch.log(input) - (1 - target) * weights[0] * torch.log(1 - input)\n",
    "        return torch.mean(bce)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our weighs\n",
    "w = 0.05\n",
    "weights = torch.tensor([w, 1-w], dtype=torch.float)\n",
    "\n",
    "# Define our model, criterion and optimizer\n",
    "model = EmbeddingsClassifier(vocab_size=np.max(list(items_set)) + 1,\n",
    "                             embedding_dim=5,)\n",
    "criterion = BCELoss_class_weighted(weights=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(pd.get_dummies(items_train, columns=[\"basket_items\"], dtype=np.int16).values)\n",
    "y_train = torch.from_numpy(items_train[\"label\"].values.astype(np.int16)).long()\n",
    "x_val = torch.from_numpy(pd.get_dummies(items_val, columns=[\"basket_items\"], dtype=np.int16).values)\n",
    "y_val = torch.from_numpy(items_val[\"label\"].values.astype(np.int16)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import training_loop\n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(x_train, y_train), batch_size=256, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_dataloader = DataLoader(TensorDataset(x_val, y_val), batch_size=256, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "model, training_loss, validate_loss, embeddings_hist = training_loop(\n",
    "            model=model, \n",
    "            criterion=criterion, \n",
    "            optimizer=optimizer, \n",
    "            train_dataloader=train_dataloader, \n",
    "            val_dataloader=val_dataloader, \n",
    "            num_epochs=3,\n",
    "            device=torch.device(\"cuda\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_one_hot_encoding(item_list: list) -> torch.Tensor:\n",
    "    one_hot_encoding = torch.zeros(np.max(list(items_set)) + 1)\n",
    "    for item in item_list:\n",
    "        one_hot_encoding[item] = 1\n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_torch_one_hot_encoding(x: np.ndarray) -> torch.Tensor:\n",
    "    return torch.stack([return_one_hot_encoding(item_list) for item_list in x[:, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Binary Classifier\n",
    "class BinaryClassifier(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 regular_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 dropout_prob: float):\n",
    "        super(BinaryClassifier).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = torch.nn.Linear(embedding_dim + regular_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, \n",
    "                x_items: torch.Tensor, \n",
    "                x_regular: torch.Tensor) -> torch.Tensor:\n",
    "        embeddings = self.embedding(x_items)\n",
    "        embeddings = torch.mean(embeddings, dim=1)\n",
    "        combined = torch.cat([embeddings, x_regular], dim=1)\n",
    "        x = self.fc1(combined)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule of thumb: embedding_dim = 4th root of vocab_size\n",
    "print(round((np.max(list(items_set)) + 1)**(1/4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.sum() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCELoss_class_weighted(weights):\n",
    "\n",
    "    def loss(input, target):\n",
    "        input = torch.clamp(input,min=1e-7,max=1-1e-7)\n",
    "        bce = - weights[1] * target * torch.log(input) - (1 - target) * weights[0] * torch.log(1 - input)\n",
    "        return torch.mean(bce)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our weighs\n",
    "w = 0.05\n",
    "weights = torch.tensor([w, 1-w], dtype=torch.float)\n",
    "\n",
    "# Define our model, criterion and optimizer\n",
    "model = BinaryClassifier(vocab_size=np.max(list(items_set)) + 1,\n",
    "                         embedding_dim=5,\n",
    "                         regular_dim=2,\n",
    "                         hidden_dim=100,\n",
    "                         dropout_prob=0.5)\n",
    "# criterion = torch.nn.BCELoss(weight=weights)\n",
    "criterion = BCELoss_class_weighted(weights=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "x_train_items = return_torch_one_hot_encoding(x_train).long()\n",
    "x_train_regular = torch.from_numpy(x_train[:, subset_indices].astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "# Validation data\n",
    "x_val_items = return_torch_one_hot_encoding(x_val).long()\n",
    "x_val_regular = torch.from_numpy(x_val[:, subset_indices].astype(np.float32))\n",
    "y_val = torch.from_numpy(y_val.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "# Test data\n",
    "x_test_items = return_torch_one_hot_encoding(x_test).long()\n",
    "x_test_regular = torch.from_numpy(x_test[:, subset_indices].astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_items.shape)\n",
    "print(x_train_regular.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our train and validation datasets\n",
    "train_dataset = TensorDataset(x_train_items, x_train_regular, y_train)\n",
    "val_dataset = TensorDataset(x_val_items, x_val_regular, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "shuffle = True\n",
    "num_workers = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=shuffle, \n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=shuffle, \n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our train step\n",
    "def train() -> float:\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for x_items, x_regular, y in train_dataloader:\n",
    "        x_items = x_items.to(device)\n",
    "        x_regular = x_regular.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_items, x_regular)\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    return train_loss / len(train_dataloader)\n",
    "\n",
    "# Define our validation step\n",
    "def validate() -> float:\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    for x_items, x_regular, y in val_dataloader:\n",
    "        x_items = x_items.to(device)\n",
    "        x_regular = x_regular.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_items, x_regular)\n",
    "        loss = criterion(y_pred, y)\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    return val_loss / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary variables\n",
    "num_epochs = 25\n",
    "best_loss = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "patience = 8  # Number of epochs to wait for improvement\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion#.to(device)\n",
    "\n",
    "embedding_hist = []\n",
    "\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train()  # Perform training steps\n",
    "    train_losses.append(train_loss)  # Track training loss\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    val_loss = validate()  # Perform validation steps\n",
    "    val_losses.append(val_loss)  # Track validation loss\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_since_improvement = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "    \n",
    "    # Check if early stopping criteria met\n",
    "    if epochs_since_improvement > patience:\n",
    "        print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "    embedding_hist.append(model.embedding.weight.data)\n",
    "\n",
    "# Load the best model checkpoint\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(val_losses, label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(embedding_hist) - 1):\n",
    "    print(embedding_hist[i] == embedding_hist[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model(x_test_items.to(device), x_test_regular.to(device)).detach().cpu().numpy()\n",
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0).squeeze()\n",
    "# y_test = y_test.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "print(\"f1:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(x: np.ndarray, labels: np.ndarray):\n",
    "    # Instantiate our PCA object\n",
    "    pca = PCA(n_components=3)\n",
    "\n",
    "    # Fit our PCA object to the data\n",
    "    pca_results = pca.fit_transform(x)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Create a dataframe for the reduced data\n",
    "    pca_df = pd.DataFrame(data=pca_results, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "\n",
    "    # Collect the data\n",
    "    xs = pca_df[\"PC1\"]\n",
    "    ys = pca_df[\"PC2\"]\n",
    "    zs = pca_df[\"PC3\"]\n",
    "    c = labels\n",
    "\n",
    "    # Plot the data\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    ax.scatter(xs, ys, zs, c=c, cmap=\"viridis\", alpha=0.5)\n",
    "\n",
    "    # Apply alpha only to a subset of data points\n",
    "    # Define the condition to select the subset\n",
    "    ax.scatter(xs[labels], ys[labels], zs[labels], c=labels, cmap='hot', alpha=1.0)\n",
    "\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_zlabel(\"PC3\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = Basket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros(203)\n",
    "for i in [basket.items_list.index(i) for i in basket.departments_to_items[\"Meat/Seafood\"]]:\n",
    "    labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(model.embedding.weight.detach().cpu().numpy(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(x: np.ndarray, labels: np.ndarray):\n",
    "    # Instantiate our T-SNE object\n",
    "    tsne = TSNE(n_components=3, learning_rate='auto')\n",
    "\n",
    "    # Fit our T-SNE object to the data\n",
    "    tsne_results = tsne.fit_transform(x)\n",
    "\n",
    "    # Create a dataframe for the reduced data\n",
    "    tsne_df = pd.DataFrame(data=tsne_results, columns=[\"dim1\", \"dim2\", \"dim3\"])\n",
    "    \n",
    "    # Collect the data\n",
    "    xs = tsne_df[\"dim1\"]\n",
    "    ys = tsne_df[\"dim2\"]\n",
    "    zs = tsne_df[\"dim3\"]\n",
    "    c = labels\n",
    "\n",
    "    # Plot the data\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    ax.scatter(xs, ys, zs, c=c, cmap=\"viridis\")\n",
    "    \n",
    "    ax.set_xlabel(\"dim1\")\n",
    "    ax.set_ylabel(\"dim2\")\n",
    "    ax.set_zlabel(\"dim3\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(model.embedding.weight.detach().cpu().numpy(), labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basket-project-beu8-sFN-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
